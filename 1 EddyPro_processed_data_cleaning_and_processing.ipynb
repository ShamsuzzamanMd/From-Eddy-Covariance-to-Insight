{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"language_info":{"name":"python","version":"3.10.18","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"────────────────────────────────────────────────────────────\n# 🐍 <font color = green> **Python-Based Script (Runtime Type)**<font/>\n\n\n────────────────────────────────────────────────────────────\n","metadata":{"id":"cjqPDi-aTuF6"}},{"cell_type":"markdown","source":"# █▓▒░ 📦 **1 Python Packages Installation** ░▒▓█\n","metadata":{"id":"o_kPyZk9hSVZ"}},{"cell_type":"code","source":"!pip install pyffp\n!pip install geopy\n!pip install geojsoncontour\n!pip install tqdm\n!pip install semopy\n!pip install graphviz\n!pip install pydotplus\n!sudo apt-get install graphviz\n!pip install windrose","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yInNWArUgxna","outputId":"8657d53f-b2e3-4f5e-f19e-7d5d9f85e645","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# █▓▒░ 📦 **2 Importing Python Packages** ░▒▓█","metadata":{"id":"pxdU3id2UedC"}},{"cell_type":"code","source":"import os\nimport sys\nimport numpy as np\nimport pandas as pd\nfrom numpy import nan\nimport time\nimport math\n\nimport matplotlib.dates as mdates  # Importing mdates module\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport geopandas as gpd\nimport matplotlib.cm as cm\nfrom pyffp import calc_footprint_FFP_climatology as myfootprint\nfrom pyffp.utils import get_dd\nfrom pyffp.utils import contour_to_gdf\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom scipy.stats import ttest_ind, ttest_rel\n\nimport semopy\nimport matplotlib.pyplot as plt\nfrom IPython.display import Image\nfrom semopy import Model  # Only use the `Model` class\nfrom graphviz import Digraph\n\nfrom windrose import WindroseAxes\n\nfrom tqdm import tqdm  # Import tqdm\n#from google.colab import files\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"id":"SvePUM34gztb","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# █▓▒░ 📦 **3 Data Reading** ░▒▓█","metadata":{"id":"hlsxvRMnhU2R"}},{"cell_type":"markdown","source":"## 【🌿 CO₂】 **3.1 Full Output and Biomet Data**\n","metadata":{"id":"QO79ApX4hn2g"}},{"cell_type":"code","source":"# Unit rows sepearation (first 3 rows) for Google Earth Engine\nco2_flux_2021_unit = pd.read_csv(\"eddypro_2021_full_output_2024-03-21T173624_adv.csv\")        # flux data\nunits =co2_flux_2021_unit.iloc[:2, :]\n\nbiomet_2021_unit = pd.read_csv(\"eddypro_2021_biomet_2024-03-20T231200_adv.csv\")       # biomet file units\nbiomet_units = biomet_2021_unit.iloc[:1, :]\n\n\n# Flux\nco2_flux_2021 = pd.read_csv(\"eddypro_2021_full_output_2024-03-21T173624_adv.csv\", skiprows =[0,2])\n\n\n# Biomet\nco2_biomet_2021 = pd.read_csv(\"eddypro_2021_biomet_2024-03-20T231200_adv.csv\", skiprows =[1])","metadata":{"id":"hd1KsIm5hXFh","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# █▓▒░ 📦 **4 Data processing** ░▒▓█","metadata":{"id":"p7P9XZnBiAci"}},{"cell_type":"markdown","source":"## 【🕒】 **4.1 Datetime Formatting**\n","metadata":{"id":"CdMjBZNciD8h"}},{"cell_type":"code","source":"co2_flux_2021['datetime'] = pd.to_datetime(co2_flux_2021[\"date\"] + ' ' + co2_flux_2021['time'], format='%Y-%m-%d %H:%M')\nco2_biomet_2021['datetime'] = pd.to_datetime(co2_biomet_2021[\"date\"] + ' ' + co2_biomet_2021['time'], format='%Y-%m-%d %H:%M')\n\n# Set datetime as index for Flux data\nco2_flux_2021.set_index('datetime', inplace=True)\n# Set datetime as index for Biomet data\nco2_biomet_2021.set_index('datetime', inplace=True)","metadata":{"id":"tIuF-T5AiBC6","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 【🔗】 **2.3 Concatenation (Flux & Biomet Data)**\n","metadata":{"id":"y5mFj9RxiO0B"}},{"cell_type":"code","source":"# Concatenate concatenated Flux and Biomet DataFrames along columns\nco2_df = pd.concat([co2_flux_2021, co2_biomet_2021], axis=1)\n# Print the shape of the merged DataFrame\nprint(\"Merged DataFrame Shape:\", co2_df.shape)\n\n# data measurement timeframe\nprint(f\"Data measurement starts on: {co2_df.index.min()}\")\nprint(f\"Data measurement ends on: {co2_df.index.max()}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Op5ls647iKvD","outputId":"a5a7d275-48c7-4727-d2cc-ecbc63ad3402","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 🔍 **2.4 Data Continuity Check**\n\n","metadata":{"id":"LS9NK5u9igC5"}},{"cell_type":"markdown","source":"**Continuous Half-hourly data checking: Checking the presence of continuous half-hourly interval (optional: possibility to see the missing measurements)**","metadata":{"id":"zA-VIiDdii6b"}},{"cell_type":"code","source":"# Checking the presence of continuous hlf-hourly data\nstart_time = co2_df.index.min()\nend_time = co2_df.index.max()\n# Check for half-hour intervals\nexpected_interval = pd.Timedelta(minutes=30)\ncurrent_time = start_time\nmissing_intervals = False  # Flag to indicate missing intervals\nwhile current_time < end_time:\n    if current_time not in co2_df.index:\n        missing_intervals = True\n        break\n    current_time += expected_interval\n# Check if missing_intervals flag is True or False\nif missing_intervals:\n    print(\"\\nNot every half-hour interval is present in the dataset index.\")\nelse:\n    print(\"\\nEvery half-hour interval is present in the dataset index.\")\n\n# Checking missing values\n#print(f\"\\nMissing values:\\n {list(co2_df.isnull().sum().items())}\", end='')                   # missing measurements","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-XOIt8COiY46","outputId":"3176ceb4-2222-4977-9468-6dec542c5697","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Creation of continuous half-hourly interval**","metadata":{"id":"YqooUsQ_ioOY"}},{"cell_type":"code","source":"print(f\"Measurement starts: {co2_df.index.min()}\")\nprint(f\"Measurement ends: {co2_df.index.max()}\")\n\n# Creating a continuous half-hourly data\n# Create a continuous half-hourly datetime index\nstart_datetime = co2_df.index.min()\nend_datetime = co2_df.index.max()\nhalf_hourly_index = pd.date_range(start=start_datetime, end=end_datetime, freq='30min')\n# Create a DataFrame with the continuous half-hourly index and NaN values in the NEE column\ndf_continuous = pd.DataFrame(index=half_hourly_index)\n# Print the minimum and maximum timestamps of the continuous index\nprint(\"\\nMinimum Timestamp (Continuous):\", df_continuous.index.min())\nprint(\"Maximum Timestamp (Continuous):\", df_continuous.index.max())\nprint(f\"\\nShape of the CO2 FluxBiomet data: {df_continuous.shape}\")\n\n# Concating the CO2 FluxBiomet data and Continuous half-hourly interval\ndf_continuous_30mins = pd.concat([co2_df, df_continuous], axis=1)\ndf_continuous_30mins.shape\n\n# Making a copy of the FluxBiomet dataset\nco2_df = df_continuous_30mins.copy()\n\n# Checking the presence of continuous hlf-hourly data\nstart_time = co2_df.index.min()\nend_time = co2_df.index.max()\n# Check for half-hour intervals\nexpected_interval = pd.Timedelta(minutes=30)\ncurrent_time = start_time\nmissing_intervals = False  # Flag to indicate missing intervals\nwhile current_time < end_time:\n    if current_time not in co2_df.index:\n        missing_intervals = True\n        break\n    current_time += expected_interval\n# Check if missing_intervals flag is True or False\nif missing_intervals:\n    print(\"\\nNot every half-hour interval is present in the dataset index.\")\nelse:\n    print(\"\\nEvery half-hour interval is present in the dataset index.\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"65AkeeRVil2T","outputId":"a9ebe21e-0eae-4f24-bd57-d4a35df5c632","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Data saving (if needed)\nco2 = co2_df.copy()\nco2_df.to_csv(\"1 EC flux-biomet concatenated data.csv\")","metadata":{"id":"t3y6Hh5AirSK","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 【🧩】 **2.5 New Columns Creation**","metadata":{"id":"k-CLHDKWi0ra"}},{"cell_type":"code","source":"co2_df['Year'] = co2_df.index.year                                              # Extract the year from the datetime objects and store it in a new column \"Year\"\nco2_df['Month'] = co2_df.index.month                                            # Month as a new column\nco2_df['DoY'] = co2_df.index.dayofyear                                          # Day of the year (DoY) as a new column\nco2_df['Hour'] = co2_df.index.hour + co2_df.index.minute / 60                   # Extract hour and minute\nco2_df['Hour_FFP'] = co2_df.index.hour                                          # Hour_FFP as a new column (specially used in Kljun Flux footprint prediction)\nco2_df['Minute'] = co2_df.index.minute                                          # Minutes","metadata":{"id":"6VQdYyBBixhy","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 【🧹】 **2.6 Duplicated Columns and Dropping Them**","metadata":{"id":"SqR_loOtjAKp"}},{"cell_type":"code","source":"# Find duplicated column names\nduplicated_columns = co2_df.columns[co2_df.columns.duplicated()].tolist()\nprint(\"Duplicated columns:\", duplicated_columns)\n\n# Drop one of the duplicated columns\nco2_merged_df = co2_df.loc[:, ~ co2_df.columns.duplicated()]\nprint(\"After dropping the duplicated (one of them is kept) columns\")\n\n# Display the DataFrame after dropping duplicated columns\n#co2_merged_df.head()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-9Q5BEAfi9yb","outputId":"6a60e619-7c23-42a8-f3c7-872594d240ce","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 💧 **2.7 Missing Values ('-9999') → np.nan**\n","metadata":{"id":"1NZpaP1yjIQB"}},{"cell_type":"code","source":"# CO2\nco2_merged_df[co2_merged_df == -9999] = np.nan               # all -9999 values are replace by np.nan (missing values)\n#co2_merged_df.head(10)\n\nco2_df = co2_merged_df.copy()","metadata":{"id":"p7yZf1fhjEh6","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# █▓▒░ 📦 **5 Wind Rose visualization** ░▒▓█","metadata":{"id":"gI7ctemFjSzx"}},{"cell_type":"code","source":"# Ensure that your DataFrame (co2_merged_df) includes columns \"Year\", \"wind_dir\", and \"u*\"\n# Drop rows with missing values\ndata = co2_merged_df[[\"Year\", \"wind_dir\", \"u*\"]].dropna()\n\n# Define consistent bins for friction velocity (u*)\nfixed_bins = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6]  # Adjust these bins as necessary\n\n# Create a single wind rose plot\nfig = plt.figure(figsize=(6, 6))\nax = WindroseAxes.from_ax(fig.add_subplot(1, 1, 1, projection=\"windrose\"))\n\n# Plot the wind rose\nbars = ax.bar(data['wind_dir'], data['u*'],\n              normed=True, opening=0.8, edgecolor='white', cmap=plt.cm.jet, bins=fixed_bins)\n\n# Add title and legend\nax.set_title(\"Wind Rose\", fontsize=15, weight='bold', pad=20)\nplt.legend(handles=ax.patches,\n           labels=[f\"[{fixed_bins[i]} ; {fixed_bins[i+1]}]\" for i in range(len(fixed_bins)-1)] + [f\">{fixed_bins[-1]}\"],\n           title=\"Friction Velocity (u*) (m/s)\", loc='lower center', bbox_to_anchor=(0.5, -0.19), ncol=len(fixed_bins))\n\n# Show plot\nplt.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":623},"id":"qDtesNzajWvS","outputId":"8eef8213-a8be-4c2a-e06f-f00753d93637","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n\n# █▓▒░ 📦 **6  Data screening and filtering** ░▒▓█","metadata":{"id":"sygMApNXjg2B"}},{"cell_type":"markdown","source":"# ⚙️ **6.1 Filter 1:**\n**CO₂ Flux Filtering Based on Bad Measurements**\n\n### 🎯 **Objective**\n\nIdentify and replace invalid carbon dioxide (CO₂) flux data (`co2_flux`) with `NaN` values based on predefined quality flags:\n- `0` → Good data  \n- `1` → Bad data  \n- `9` → Missing data  \n\n### 📊 **Background**\n\nThe dataset includes three quality-control columns:  \n`spikes_df`, `amplitude_resolution_hf`, and `absolute_limits_hf`.  \nEach column contains nine-digit codes (e.g., `800000099` or `800000099.0`), where each digit represents a specific parameter — `u`, `v`, `w`, `ts`, `co2`, `h2o`, `ch4`, and `none`.\n\nThese codes are separated into individual columns such as:\n`spikes_hf_u`, `spikes_hf_co2`, `absolute_limits_hf_none`, etc., for easier interpretation.\n\n### ⚙️ **Process**\n\n1. **Redistribution:**  \n   Split and assign the coded QC values into new columns according to their parameters.\n\n2. **Replacement:**  \n   Substitute CO₂ flux values (`co2_flux`) with `NaN` wherever the related flag equals `1`, indicating a poor-quality measurement.\n\n---\n\n💡 *This step ensures that subsequent analyses only use reliable CO₂ flux data, improving the accuracy of flux filtering and carbon budget estimates.*","metadata":{"id":"6c4D165-jkcp"}},{"cell_type":"code","source":"#df[\"spikes_hf\"]                          # 8u/v/w/ts/co2/h2o/ch4/none\n#df[\"amplitude_resolution_hf\"]            # 8u/v/w/ts/co2/h2o/ch4/none\n#df[\"absolute_limits_hf\"]                  # 8u/v/w/ts/co2/h2o/ch4/none","metadata":{"id":"JID_AgbnZn4E","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4.1.1 Missing values checking before filtering","metadata":{"id":"CkNh77PjjolZ"}},{"cell_type":"code","source":"","metadata":{"id":"dHVRGnSdjdWC","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ⚙️ **6.1.1 Spikes_hf (#8u / #v / #w / #ts / #co2 / #h2o / #ch4 / #none)**\n","metadata":{"id":"x6mTFvF3jz2R"}},{"cell_type":"code","source":"mv_b_1st_filter = co2_df[\"co2_flux\"].isnull().sum()\nprint(mv_b_1st_filter)\n\n# Slice each string from the \"spikes_hf\" column and split it into individual characters\nsplit_values = co2_df[\"spikes_hf\"].astype(str).str[1:9].apply(lambda x: list(x))\n\n# Convert the result to a DataFrame\nsplit_values_df = pd.DataFrame(split_values.tolist(), index=co2_df.index, columns=['spikes_hf_u', 'spikes_hf_v', 'spikes_hf_w', 'spikes_hf_ts', 'spikes_hf_co2', 'spikes_hf_h2o', 'spikes_hf_ch4', 'spikes_hf_none']).apply(pd.to_numeric, errors='coerce')\n\n# Create a mask for NaN values in the \"spikes_hf\" column\nnan_mask = co2_df[\"spikes_hf\"].isna()\n\n# Assign NaN values to the entire DataFrame where \"spikes_hf\" column is NaN\nsplit_values_df[nan_mask] = np.nan\n\n# Update the original DataFrame with the new values\nco2_df[['spikes_hf_u', 'spikes_hf_v', 'spikes_hf_w', 'spikes_hf_ts', 'spikes_hf_co2', 'spikes_hf_h2o', 'spikes_hf_ch4', 'spikes_hf_none']] = split_values_df[['spikes_hf_u', 'spikes_hf_v', 'spikes_hf_w', 'spikes_hf_ts', 'spikes_hf_co2', 'spikes_hf_h2o', 'spikes_hf_ch4', 'spikes_hf_none']]\n\n# Display the updated DataFrame\n#co2_df.head()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PgfJqgZVjwMS","outputId":"17dc1c11-f874-45f2-89ad-2d23fe03d2f0","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ⚙️ **6.1.2 Amplitude_resolution_hf (#8u / #v / #w / #ts / #co2 / #h2o / #ch4 / #none)**\n","metadata":{"id":"jLN3WRg9j7hB"}},{"cell_type":"code","source":"# Slice each string from the \"amplitude_resolution_hf\" column and split it into individual characters\nsplit_values_amplitude = co2_df[\"amplitude_resolution_hf\"].astype(str).str[1:9].apply(lambda x: list(x))\n\n# Convert the result to a DataFrame\nsplit_values_amplitude_df = pd.DataFrame(split_values_amplitude.tolist(), index=co2_df.index, columns=['amplitude_resolution_hf_u', 'amplitude_resolution_hf_v', 'amplitude_resolution_hf_w', 'amplitude_resolution_hf_ts', 'amplitude_resolution_hf_co2', 'amplitude_resolution_hf_h2o', 'amplitude_resolution_hf_ch4', 'amplitude_resolution_hf_none']).apply(pd.to_numeric, errors='coerce')\n\n# Create a mask for NaN values in the \"amplitude_resolution_hf\" column\nnan_mask_amplitude = co2_df[\"amplitude_resolution_hf\"].isna()\n\n# Assign NaN values to the entire DataFrame where \"amplitude_resolution_hf\" column is NaN\nsplit_values_amplitude_df[nan_mask_amplitude] = np.nan\n\n# Update the original DataFrame with the new values\nco2_df[['amplitude_resolution_hf_u', 'amplitude_resolution_hf_v', 'amplitude_resolution_hf_w', 'amplitude_resolution_hf_ts', 'amplitude_resolution_hf_co2', 'amplitude_resolution_hf_h2o', 'amplitude_resolution_hf_ch4', 'amplitude_resolution_hf_none']] = split_values_amplitude_df[['amplitude_resolution_hf_u', 'amplitude_resolution_hf_v', 'amplitude_resolution_hf_w', 'amplitude_resolution_hf_ts', 'amplitude_resolution_hf_co2', 'amplitude_resolution_hf_h2o', 'amplitude_resolution_hf_ch4', 'amplitude_resolution_hf_none']]\n\n# Display the updated DataFrame\n#co2_df.head()","metadata":{"id":"2ik7lqVVj4sy","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ⚙️ **6.1.3 Absolute_limits_hf (#8u / #v / #w / #ts / #co2 / #h2o / #ch4 / #none)**\n","metadata":{"id":"PqIip-likBCJ"}},{"cell_type":"code","source":"# Slice each string from the \"absolute_limits_hf\" column and split it into individual characters\nsplit_values_absolute = co2_df[\"absolute_limits_hf\"].astype(str).str[1:9].apply(lambda x: list(x))\n\n# Convert the result to a DataFrame\nsplit_values_absolute_df = pd.DataFrame(split_values_absolute.tolist(), index=co2_df.index, columns=['absolute_limits_hf_u', 'absolute_limits_hf_v', 'absolute_limits_hf_w', 'absolute_limits_hf_ts', 'absolute_limits_hf_co2', 'absolute_limits_hf_h2o', 'absolute_limits_hf_ch4', 'absolute_limits_hf_none']).apply(pd.to_numeric, errors='coerce')\n\n# Create a mask for NaN values in the \"absolute_limits_hf\" column\nnan_mask_absolute = co2_df[\"absolute_limits_hf\"].isna()\n\n# Assign NaN values to the entire DataFrame where \"absolute_limits_hf\" column is NaN\nsplit_values_absolute_df[nan_mask_absolute] = np.nan\n\n# Update the original DataFrame with the new values\nco2_df[['absolute_limits_hf_u', 'absolute_limits_hf_v', 'absolute_limits_hf_w', 'absolute_limits_hf_ts', 'absolute_limits_hf_co2', 'absolute_limits_hf_h2o', 'absolute_limits_hf_ch4', 'absolute_limits_hf_none']] = split_values_absolute_df[['absolute_limits_hf_u', 'absolute_limits_hf_v', 'absolute_limits_hf_w', 'absolute_limits_hf_ts', 'absolute_limits_hf_co2', 'absolute_limits_hf_h2o', 'absolute_limits_hf_ch4', 'absolute_limits_hf_none']]\n\n# Display the updated DataFrame\n#co2_df.head()","metadata":{"id":"RUINc1Ayj-5S","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 📊 **6.1.4 Measurements_distribution_exploration**\n","metadata":{"id":"3YxeUbQYkFxZ"}},{"cell_type":"code","source":"# List of column names\ncolumns = ['spikes_hf_u', 'spikes_hf_v', 'spikes_hf_w', 'spikes_hf_co2', 'absolute_limits_hf_u', 'absolute_limits_hf_v', 'absolute_limits_hf_w', 'absolute_limits_hf_co2', 'amplitude_resolution_hf_u', 'amplitude_resolution_hf_v', 'amplitude_resolution_hf_w', 'amplitude_resolution_hf_co2']\n\ntotal_one = sum((co2_df[col] == 1).sum() for col in columns)\nfor col in columns:\n    print(co2_df[col].value_counts())\n    print(\"\\n\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5LGTtzFmkDya","outputId":"60fd7f9b-4c56-4b32-8262-62d5ee77fd25","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 🧯 **6.1.5 Replacing CO₂ Flux with NaN Values Due to Instrument Measurement Errors**\n","metadata":{"id":"xGpUp8RBkeoa"}},{"cell_type":"code","source":"# Create a mask for rows where any of the specified columns is equal to 1\nrowwise_mask = (co2_df[['spikes_hf_u', 'spikes_hf_v', 'spikes_hf_w', 'spikes_hf_co2', 'absolute_limits_hf_u', 'absolute_limits_hf_v', 'absolute_limits_hf_w', 'absolute_limits_hf_co2', 'amplitude_resolution_hf_u', 'amplitude_resolution_hf_v', 'amplitude_resolution_hf_w', 'amplitude_resolution_hf_co2']] == 1).any(axis=1)\n\n# Replace values in co2_flux column with NaN where any of the specified columns is equal to 1\nco2_df.loc[rowwise_mask, 'co2_flux'] = np.nan","metadata":{"id":"mkiFjlCzkH-D","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 📉 **6.1.6 Amount of Missing Values Checking After Filtering**\n","metadata":{"id":"JmlsU9bWkjmi"}},{"cell_type":"code","source":"mv_a_1st_filter = co2_df[\"co2_flux\"].isnull().sum()\nprint(mv_a_1st_filter)\n\nprint(\"\\n\")\nprint(f\"Increase in missing values: {mv_a_1st_filter - mv_b_1st_filter}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C_uhl2pAkhZT","outputId":"bddf8897-c9bb-4266-b50f-1a55a1e90424","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ⚙️ **6.2 Filter 2:**\n**Filtering based on qc_co2_flux flag**","metadata":{"id":"nk3hKdtGkpi5"}},{"cell_type":"markdown","source":"### 🎯 **Objective**\n\nThe goal of this section is to ensure data quality by filtering out unreliable measurements.\n\nSpecifically:\n- We filter the **CO₂ flux (`co2_flux`)** and **latent heat (`LE`)** data using the quality control variable **`qc_co2_flux`**.  \n- Any record where **`qc_co2_flux` > 1** is considered low-quality and therefore replaced with **`NaN`** values in both `co2_flux` and `LE`.\n\nThis step helps maintain consistency and reliability in subsequent analyses.\n","metadata":{"id":"RIAZyZLCkt46"}},{"cell_type":"markdown","source":"","metadata":{"id":"umg5ebDWkyax"}},{"cell_type":"markdown","source":"## 🧹 **6.2.1 Filtering and Replacing `qc_co2_flux` > 1.0 with NaN**\n","metadata":{"id":"VunIYMaDk5oq"}},{"cell_type":"code","source":"mv_b_2nd_filter = co2_df[\"co2_flux\"].isnull().sum() #Amount of missing values checking before filtering\nprint(mv_b_2nd_filter)\n\n# Replace values in 'NEE' column where 'qc_co2_flux' > 1.0 with NaN\nco2_df.loc[co2_df['qc_co2_flux'] > 1.0, 'co2_flux'] = np.nan\n# Replace values in 'LE' column where 'qc_co2_flux' > 1.0 with NaN\nco2_df.loc[co2_df['qc_co2_flux'] > 1.0, 'LE'] = np.nan\n\nmv_a_2nd_filter = co2_df[\"co2_flux\"].isnull().sum()\nprint(mv_a_2nd_filter)\n\nprint(\"\\n\")\nprint(f\"Increase in missing values: {mv_a_2nd_filter - mv_b_2nd_filter}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5WH1trlnk1LD","outputId":"bd27a7a4-a59c-48f3-fe8c-39ade160a698","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 🌍 **6.3 Filter 3: Footprint Filtering with Google Earth Engine Using EC Fetch Distances**","metadata":{"id":"hi4dlhMddjpD"}},{"cell_type":"markdown","source":"This section focuses on filtering the **CO₂ flux (`co2_flux`)** based on the **flux footprint area**, commonly referred to as the *fetch*.\n\nUsing **Google Earth Engine (GEE)** integrated with **Python**, the method calculates distances corresponding to **relative footprint contributions** at **50%, 60%, 70%, and 80%** levels from the EddyPro full output data.\n\n### ⚙️ **Process Overview**\n\n1. **Generate a fetch dataset:**  \n   Create an Excel file listing distances from the **Eddy Covariance (EC) Tower** to the edge of the footprint (*fetch*), measured every **10°**.\n\n2. **Save the dataset:**  \n   Store the complete Excel file for reference and later integration.\n\n3. **Apply the filter:**  \n   Use the fetch distance data and model outputs to **filter `co2_flux`** values, retaining only those within the defined footprint boundaries.","metadata":{"id":"a7rral_mlMNB"}},{"cell_type":"code","source":"print(f\"Missing values before applying fetch filter: {co2_df['co2_flux'].isnull().sum()}\")\n\n# Load the fetch parameters file (fetches.xlsx)\nfetches_df = pd.read_excel('fetches.xlsx')\n\n# Load the dataset that contains co2_flux, wind_dir, and x_70%\ndata_df = co2_df.copy()\n# Iterate through the dataset and compare with fetches data\nfor index, row in data_df.iterrows():\n    wind_dir = row['wind_dir']\n    x_70_percent = row['x_70%']\n\n    # Iterate through the fetches file to find matching wind_dir range\n    for _, fetch_row in fetches_df.iterrows():\n        start_wd = fetch_row['Start_WD ']\n        end_wd = fetch_row['End_WD']\n        fetch_dist = fetch_row[' Fetch ']\n\n        # Check if wind_dir falls in the range [Start_WD, End_WD]\n        if start_wd <= wind_dir <= end_wd:\n            # Check if the x_70% distance is more than Fetch distance\n            if x_70_percent > fetch_dist:\n                # Replace co2_flux with NaN\n                data_df.at[index, 'co2_flux'] = np.nan\n            # Break the loop once the match is found\n            break\n\n# Save the modified dataset (if needed)\ndata_df.to_csv('final.csv', index=False)  # Replace with your desired save path\n\n\n\nprint(f\"Missing values after applying fetch filter: {data_df['co2_flux'].isnull().sum()}\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6RSqEqoHlJeT","outputId":"b7a1d86e-adc5-41d5-e2d1-1dbc2b7be782","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"co2_df = data_df.copy()                                # The filtered dataset is \"data_df\"","metadata":{"id":"99S-A2CplQdT","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# █▓▒░ 📦 **7  Exploratory Data Analysis (EDA) part 1** ░▒▓█","metadata":{"id":"eXi2A4UBle6B"}},{"cell_type":"markdown","source":"This section provides a concise summary of key statistical metrics — including the **minimum**, **maximum**, **mean**, and **median** — derived from the primary flux and storage variables: **CO₂**, **latent heat (LE)**, and **sensible heat (H)**.\n\nThese statistics offer a quick overview of data distribution and variability, helping to identify potential outliers or measurement inconsistencies before further analysis.","metadata":{"id":"29J0L54blkQV"}},{"cell_type":"markdown","source":"## 🏭 **7.1 CO₂ Storage**\n","metadata":{"id":"TjlHNwwTlnOq"}},{"cell_type":"code","source":"cols = ['co2_strg', 'LE_strg', 'H_strg', 'co2_flux', 'LE', 'H']\ncol_stat = co2_df[cols].describe().round(2)\ncol_stat","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":301},"id":"UdlkUrTKfN_H","outputId":"403ed721-3589-4ed9-f587-c4c049581e0f","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# █▓▒░ 📦 **8 Flux storage correction** ░▒▓█","metadata":{"id":"PLoe-uKEmHWr"}},{"cell_type":"markdown","source":"This section aims to achieve the following objectives:\n\n1. Incorporate storage changes into CO2, latent heat (LE), and sensible heat (H) fluxes. Any missing storage change values are standardized to 0.\n2. Calculate Net Ecosystem Exchange (NEE), total latent heat (LE), and total sensible heat (H) by aggregating fluxes and storage changes.\n\nIn essence, these codes ensure that storage changes are appropriately factored into the flux calculations, resulting in the accurate determination of total fluxes (NEE, LE, and H) through the summation of fluxes and storage changes.","metadata":{"id":"ErHlcXvKmUAx"}},{"cell_type":"code","source":"# Adding storage change to CO2 and heat fluxes (set missing storage change to 0)\nco2_df.loc[co2_df['co2_strg'].isnull(),'co2_strg'] = 0\nco2_df.loc[co2_df['LE_strg'].isnull(),'LE_strg'] = 0\nco2_df.loc[co2_df['H_strg'].isnull(),'H_strg'] = 0\n\n# Calculation of NEE, LE, and H\nco2_df['NEE'] = co2_df['co2_flux'] + co2_df['co2_strg']\nco2_df['LE'] = co2_df['LE'] + co2_df['LE_strg']\nco2_df['H'] = co2_df['H'] + co2_df['H_strg']","metadata":{"id":"UYyTW2sPmDPT","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n\n# █▓▒░ 📦 **9 Outliers Removal** ░▒▓█","metadata":{"id":"WNLok_U7mY40"}},{"cell_type":"code","source":"# Assuming co2_df is a pandas DataFrame and here are the columns of interest\ncolumns_of_interest = [\n    \"air_temperature\", \"LE\", \"H\", \"RH\", \"RH_1_1_1\",\n    \"TS_1_1_1\", \"NEE\", \"PPFD_1_1_1\", \"SWIN_1_1_1\",\n    \"ET\", \"SWC_1_1_1\", \"VPD\"\n]\n\n# Loop through each column in the list of columns of interest\nfor column in columns_of_interest:\n    # Calculate Q1 (25th percentile) and Q3 (75th percentile) for each column\n    Q1 = co2_df[column].quantile(0.25)\n    Q3 = co2_df[column].quantile(0.75)\n\n    # Calculate IQR\n    IQR = Q3 - Q1\n\n    # Define the lower and upper bounds for outliers\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n\n    # Replace values outside the bounds with np.nan\n    co2_df[column] = co2_df[column].mask((co2_df[column] < lower_bound) | (co2_df[column] > upper_bound))\n\n# Now co2_df is updated, with outliers in the specified columns replaced by NaN","metadata":{"id":"Ma3q0JaomW5T","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# █▓▒░ 📦 **10 Exploratory Data Analysis (EDA) part 2** ░▒▓█","metadata":{"id":"GIiU5s9HhCql"}},{"cell_type":"code","source":"# Generalize the name of the dataset as \"df\" for execution simplicity\ndf = co2_df.copy()","metadata":{"id":"J2E9qbQGmcGr","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 🌿 **10.1 NEE Visualization**\n","metadata":{"id":"MxRfpzXRmtxJ"}},{"cell_type":"code","source":"# --- Combined Subplots ---\nplt.figure(figsize=(20, 4))\n\n# Subplot 1: Raw NEE\nplt.subplot(1, 2, 1)\nplt.plot(df[\"NEE\"], color='forestgreen', linewidth=1)\nplt.title(\"Net Ecosystem Exchange (NEE)\", fontsize=14)\nplt.xlabel(\"\")  # omit for top plot\nplt.ylabel(r\"µmol CO$_2$ m$^{-2}$ s$^{-1}$\", fontsize=12)\nplt.grid(True, linestyle=\"--\", alpha=0.5)\n\n\n# Subplot 2: Daily Mean NEE\n# Compute daily mean\ndaily_nee = df['NEE'].resample('D').mean()\nplt.subplot(1, 2, 2)\nplt.plot(daily_nee, color='limegreen', linewidth=1.5)\nplt.title(\"Daily Mean of Net Ecosystem Exchange (NEE)\", fontsize=14)\nplt.xlabel(\"Time\", fontsize=12)\nplt.ylabel(r\"µmol CO$_2$ m$^{-2}$ s$^{-1}$\", fontsize=12)\nplt.grid(True, linestyle=\"--\", alpha=0.5)\n\nplt.tight_layout()\nplt.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":407},"id":"ERwMukdvmrmS","outputId":"9165e192-8823-4a82-fc93-84ff102c4c94","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\nprint(f\"Min: {df['NEE'].min()}\")\nprint(f\"Mean: {df['NEE'].mean()}\")\nprint(f\"Max: {df['NEE'].max()}\")\nprint(f\"Median: {df['NEE'].median()}\")\nprint(\"\\n\")\nprint(f\"Lower values\\nScreening:\\n {df['NEE'][df['NEE'] < -25]}\\n\")\nprint(f\"Screening, total: {df['NEE'][df['NEE'] < -25].value_counts().sum()}\\n\")\nprint(\"\\n\")\nprint(f\"Screening:\\n {df['NEE'][df['NEE'] > 25]}\\n\")\nprint(f\"Screening, total: {df['NEE'][df['NEE'] > 25].value_counts().sum()}\")\n'''","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"id":"My7hnFzxmwUk","outputId":"21541f3c-4c7b-4cb2-c5bb-196d7582c318","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 🌿 **10.2 H Visualization**\n","metadata":{"id":"3ZqKnMTcm9bR"}},{"cell_type":"code","source":"# --- Combined Subplots ---\nplt.figure(figsize=(20, 4))\n\n# Subplot 1: Raw Sensible Heat Flux\nplt.subplot(1, 2, 1)\nplt.plot(df[\"H\"], color='darkorange', linewidth=1)\nplt.title(\"Sensible Heat Flux (H)\", fontsize=14)\nplt.xlabel(\"\")  # remove x-label to avoid overlap\nplt.ylabel(r\"W m$^{-2}$\", fontsize=12)\nplt.grid(True, linestyle=\"--\", alpha=0.5)\n\n# Subplot 2: Daily Mean Sensible Heat Flux\n# Compute daily mean\ndaily_H = df['H'].resample('D').mean()\nplt.subplot(1, 2, 2)\nplt.plot(daily_H, color='red', linewidth=1.5)\nplt.title(\"Daily Mean Sensible Heat Flux (H)\", fontsize=14)\nplt.xlabel(\"Timestamps\", fontsize=12)\nplt.ylabel(r\"W m$^{-2}$\", fontsize=12)\nplt.grid(True, linestyle=\"--\", alpha=0.5)\n\nplt.tight_layout()\nplt.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":407},"id":"fURAhqismzhq","outputId":"9bb3ba57-4ec9-42a9-a1a0-66f34ef6be68","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\nprint(f\"Min: {df['H'].min()}\")\nprint(f\"Mean: {df['H'].mean()}\")\nprint(f\"Max: {df['H'].max()}\")\nprint(f\"Median: {df['H'].median()}\")\n\nprint(f\"Screening:\\n {df['H'][df['H'] < -20]}\\n\")\nprint(f\"Screening, total: {df['H'][df['H'] < -20].value_counts().sum()}\")\n\nprint(f\"Screening:\\n {df['H'][df['H'] > 100]}\\n\")\nprint(f\"Screening, total: {df['H'][df['H'] > 100].value_counts().sum()}\")\n'''","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"id":"Vac7EpT1m_7D","outputId":"61ed007f-a30c-4c9c-8d36-93d2b443bdab","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 🌿 **10.3 LE Visualization**\n","metadata":{"id":"op3ZTm6mnZf5"}},{"cell_type":"code","source":"# --- Plot both in subplots ---\nplt.figure(figsize=(20, 4))\n\n# Subplot 1: Raw LE\nplt.subplot(1, 2, 1)\nplt.plot(df[\"LE\"], color='teal', linewidth=1)\nplt.title(\"Latent Heat Flux (LE)\", fontsize=14)\nplt.xlabel(\"\")  # No xlabel for top plot\nplt.ylabel(r\"W m$^{-2}$\", fontsize=12)\nplt.grid(True, linestyle=\"--\", alpha=0.5)\n\n\n# Subplot 2: Daily Mean LE\n# Daily mean\ndaily_LE = df['LE'].resample('D').mean()\nplt.subplot(1, 2, 2)\nplt.plot(daily_LE, color='orange', linewidth=1.5)\nplt.title(\"Daily Mean of Latent Heat Flux (LE)\", fontsize=14)\nplt.xlabel(\"Timestamps\", fontsize=12)\nplt.ylabel(r\"W m$^{-2}$\", fontsize=12)\nplt.grid(True, linestyle=\"--\", alpha=0.5)\n\n# Layout adjustment\nplt.tight_layout()\nplt.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":407},"id":"KsrwFeq1nNFL","outputId":"088993e3-a9ed-4f5b-8a7c-659cae501d97","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\nprint(f\"Min: {df['LE'].min()}\")\nprint(f\"Mean: {df['LE'].mean()}\")\nprint(f\"Max: {df['LE'].max()}\")\nprint(f\"Median: {df['LE'].median()}\")\n\nprint(f\"Screening:\\n {df['LE'][df['LE'] < -20]}\\n\")\nprint(f\"Screening, total: {df['LE'][df['LE'] < -20].value_counts().sum()}\")\n\nprint(f\"Screening:\\n {df['LE'][df['LE'] > 80]}\\n\")\nprint(f\"Screening, total: {df['LE'][df['LE'] > 80].value_counts().sum()}\")\n'''","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"id":"Pl3ta_r2nb3i","outputId":"f52dbed0-8ba9-4f0c-d7d9-8a135e56dfc0","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 🌿 **10.4 NEE Visualization**\n","metadata":{"id":"jrTQ3PH7nmN6"}},{"cell_type":"code","source":"# --- Plot Air Temperature (Raw + Daily Mean) ---\nplt.figure(figsize=(20, 4))\n\n# 1st subplot - Raw Air Temperature\nplt.subplot(1, 2, 1)\nplt.plot(df[\"air_temperature\"], color='tab:blue')\nplt.title(\"Air Temperature (Raw)\", fontsize=14)\nplt.xlabel(\"Timestamps\", fontsize=12)\nplt.ylabel(r\"Temperature (°C)\", fontsize=12)\nplt.grid(True, linestyle=\"--\", alpha=0.5)\n\n# 2nd subplot - Daily Mean Air Temperature\ndaily_ta = df['air_temperature'].resample('D').mean()\nplt.subplot(1, 2, 2)\nplt.plot(daily_ta, color='tab:orange')\nplt.title(\"Daily Mean Air Temperature\", fontsize=14)\nplt.xlabel(\"Timestamps\", fontsize=12)\nplt.ylabel(r\"Temperature (°C)\", fontsize=12)\nplt.grid(True, linestyle=\"--\", alpha=0.5)\nplt.tight_layout()\nplt.show()\n\n\n# --- Plot Soil Temperature (Raw + Daily Mean) ---\nplt.figure(figsize=(20, 4))\n\n# Raw Soil Temperature\nplt.subplot(1, 2, 1)\nplt.plot(df[\"TS_1_1_1\"], color='tab:brown')\nplt.title(\"Soil Temperature TS_1_1_1 (Raw Data)\", fontsize=14)\nplt.xlabel(\"Time\", fontsize=12)\nplt.ylabel(\"Soil Temperature (°C)\", fontsize=12)\nplt.grid(True, linestyle=\"--\", alpha=0.5)\n\n# Daily Mean Soil Temperature\ndaily_ts = df[\"TS_1_1_1\"].resample(\"D\").mean()\nplt.subplot(1, 2, 2)\nplt.plot(daily_ts, color='tab:green')\nplt.title(\"Daily Mean Soil Temperature (TS_1_1_1)\", fontsize=14)\nplt.xlabel(\"Date\", fontsize=12)\nplt.ylabel(\"Mean Soil Temperature (°C)\", fontsize=12)\nplt.grid(True, linestyle=\"--\", alpha=0.5)\n\nplt.tight_layout()\nplt.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":797},"id":"47kA5AR3nedm","outputId":"ba6ec487-35fe-4d1f-eb35-16a81fecdb97","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert temperatures from Kelvin to Celsius\ndf['TS_1_1_1'] = df['TS_1_1_1'] - 273.15\ndf['air_temperature'] = df['air_temperature'] - 273.15","metadata":{"id":"58JunESRZ-4P","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 🌿 **10.5 Relative humidity (RH and RH_1_1_1) Visualization**","metadata":{"id":"N80V9ZpVnq4C"}},{"cell_type":"code","source":"# Compute daily mean\ndaily_RH = df['RH'].resample('D').mean()\n\n# --- Combined Subplots ---\nplt.figure(figsize=(20, 4))\n\n# Subplot 1: Raw RH\nplt.subplot(1, 2, 1)\nplt.plot(df[\"RH\"], color='royalblue', linewidth=1)\nplt.title(\"Relative Humidity (RH)\", fontsize=14)\nplt.xlabel(\"\")  # omit to prevent overlap\nplt.ylabel(\"Relative Humidity (%)\", fontsize=12)\nplt.grid(True, linestyle=\"--\", alpha=0.5)\n\n# Subplot 2: Daily Mean RH\nplt.subplot(1, 2, 2)\nplt.plot(daily_RH, color='deepskyblue', linewidth=1.5)\nplt.title(\"Daily Mean of Relative Humidity (RH)\", fontsize=14)\nplt.xlabel(\"Timestamps\", fontsize=12)\nplt.ylabel(\"Relative Humidity (%)\", fontsize=12)\nplt.grid(True, linestyle=\"--\", alpha=0.5)\n\nplt.tight_layout()\nplt.show()\n\n\n\n# RH_1_1_1\n# Compute daily mean\ndaily_RH = df['RH_1_1_1'].resample('D').mean()\n\n# --- Combined Subplots ---\nplt.figure(figsize=(20, 4))\n\n# Subplot 1: Raw RH\nplt.subplot(1, 2, 1)\nplt.plot(df[\"RH_1_1_1\"], color='royalblue', linewidth=1)\nplt.title(\"Relative Humidity (RH_1_1_1)\", fontsize=14)\nplt.xlabel(\"\")  # omit to prevent overlap\nplt.ylabel(\"Relative Humidity (%)\", fontsize=12)\nplt.grid(True, linestyle=\"--\", alpha=0.5)\n\n# Subplot 2: Daily Mean RH\nplt.subplot(1, 2, 2)\nplt.plot(daily_RH, color='deepskyblue', linewidth=1.5)\nplt.title(\"Daily Mean of Relative Humidity (RH_1_1_1)\", fontsize=14)\nplt.xlabel(\"Timestamps\", fontsize=12)\nplt.ylabel(\"Relative Humidity (%)\", fontsize=12)\nplt.grid(True, linestyle=\"--\", alpha=0.5)\n\nplt.tight_layout()\nplt.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":797},"id":"LIn7le2anoRb","outputId":"735b2fef-4281-4089-c0dd-787592f7fad7","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 🌿 **10.6 Shortwave Incoming Radiation (SWIN) Visualization**","metadata":{"id":"JNgTZm54il-X"}},{"cell_type":"code","source":"# Compute daily mean\ndaily_swin = df['SWIN_1_1_1'].resample('D').mean()\n\n# --- Combined Subplots ---\nplt.figure(figsize=(20, 4))\n\n# Subplot 1: Raw Incoming Shortwave Radiation\nplt.subplot(1, 2, 1)\nplt.plot(df[\"SWIN_1_1_1\"], color='gold', linewidth=1)\nplt.title(\"Incoming Shortwave Radiation (SWIN)\", fontsize=14)\nplt.xlabel(\"\")  # omit x-label for top subplot\nplt.ylabel(r\"W m$^{-2}$\", fontsize=12)\nplt.grid(True, linestyle=\"--\", alpha=0.5)\n\n# Subplot 2: Daily Mean Incoming Shortwave Radiation\nplt.subplot(1, 2, 2)\nplt.plot(daily_swin, color='darkgoldenrod', linewidth=1.5)\nplt.title(\"Daily Mean of Incoming Shortwave Radiation (SWIN)\", fontsize=14)\nplt.xlabel(\"Timestamps\", fontsize=12)\nplt.ylabel(r\"W m$^{-2}$\", fontsize=12)\nplt.grid(True, linestyle=\"--\", alpha=0.5)\n\nplt.tight_layout()\nplt.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":407},"id":"LrZ-I7Lxnwey","outputId":"702bf573-c112-44ef-bf2e-285dc94c30b2","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 🌿 **10.7 VPD Visualization**","metadata":{"id":"I3kof1l9n78C"}},{"cell_type":"code","source":"# Compute daily mean\ndaily_vpd = df['VPD'].resample('D').mean()\n\n# --- Combined Subplots ---\nplt.figure(figsize=(20, 4))\n\n# Subplot 1: Raw VPD\nplt.subplot(1, 2, 1)\nplt.plot(df[\"VPD\"], color='mediumvioletred', linewidth=1)\nplt.title(\"Vapor Pressure Deficit (VPD)\", fontsize=14)\nplt.xlabel(\"\")  # omit x-label for top plot\nplt.ylabel(\"VPD (Pa)\", fontsize=12)\nplt.grid(True, linestyle=\"--\", alpha=0.5)\n\n# Subplot 2: Daily Mean VPD\nplt.subplot(1, 2, 2)\nplt.plot(daily_vpd, color='purple', linewidth=1.5)\nplt.title(\"Daily Mean of Vapor Pressure Deficit (VPD)\", fontsize=14)\nplt.xlabel(\"Timestamps\", fontsize=12)\nplt.ylabel(\"VPD (Pa)\", fontsize=12)\nplt.grid(True, linestyle=\"--\", alpha=0.5)\nplt.tight_layout()\nplt.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":407},"id":"1R__YCPJn6Wj","outputId":"3d7f4907-a889-4d98-8a52-f55e53d60f98","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Update df[\"VPD\"] where values are less than 0 to np.nan\ndf.loc[df[\"VPD\"] < 0, \"VPD\"] = np.nan                                      # VPD below 0 is assigned as np.nan\n\ndf[\"VPD\"] = df[\"VPD\"]/1000  # convert from hPa → kPa","metadata":{"id":"FOh2SGuFn-h7","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 🌿 **10.7 Soil Water Content (SWC) Visualization**","metadata":{"id":"DMsp4FB4oaMi"}},{"cell_type":"code","source":"# --- Compute daily mean ---\ndaily_swc = df['SWC_1_1_1'].resample('D').mean()\n\n# --- Side-by-side subplots ---\nfig, axs = plt.subplots(1, 2, figsize=(20, 4))\n\n# Left plot: Raw SWC\naxs[0].plot(df[\"SWC_1_1_1\"], color='saddlebrown', linewidth=1)\naxs[0].set_title(\"Soil Water Content (SWC_1_1_1)\", fontsize=14)\naxs[0].set_xlabel(\"\")\naxs[0].set_ylabel(\"SWC (m³ m⁻³)\", fontsize=12)\naxs[0].grid(True, linestyle=\"--\", alpha=0.5)\n\n# Right plot: Daily Mean SWC\naxs[1].plot(daily_swc, color='peru', linewidth=1.5)\naxs[1].set_title(\"Daily Mean of Soil Water Content (SWC_1_1_1)\", fontsize=14)\naxs[1].set_xlabel(\"Timestamps\", fontsize=12)\naxs[1].set_ylabel(\"SWC (m³ m⁻³)\", fontsize=12)\naxs[1].grid(True, linestyle=\"--\", alpha=0.5)\n\nplt.tight_layout()\nplt.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":407},"id":"H3jF5hCeaFns","outputId":"652eb639-1dd2-4d61-9453-7ede65cf5622","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['SWC_1_1_1'] = df['SWC_1_1_1'] * 100  # Soil moisture content, decimal to %","metadata":{"id":"PIZoUtKJoYRD","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#df[\"P_RAIN_1_1_1\"] = df[\"P_RAIN_1_1_1\"]*1000","metadata":{"id":"FSE8AWl9odR8","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Foot print analysis\n#ft = df.copy()","metadata":{"id":"yvSZti3roma7","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# █▓▒░ 📦 **11 Processed and clean data before Gap-filling by ReddyPro** ░▒▓█","metadata":{"id":"UwU2GfoIo3aK"}},{"cell_type":"code","source":"# Processed, filtered (all filters including the fetch distance filter), not gap-filled data\n# Define the filename for the CSV file\ndf.to_csv(\"2 EC_processed_cleaned_data.csv\")\nco2_processed_clean_data = df.copy()\nmeteorological_condition = df.copy()\nco2_processed_clean_data.to_csv(\"meteorological_condition.csv\")","metadata":{"id":"4oGko8OqpHDD","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"co2_processed_clean_data.shape","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k6E6aC40o0nT","outputId":"7e631cdc-e33d-4973-fa78-040f1ba19381","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# █▓▒░ 📦 **12 Data for Gap-filling by ReddyProc package** ░▒▓█","metadata":{"id":"Ij_p-xhrpQPa"}},{"cell_type":"markdown","source":"## 🧾 **12.1 Columns for REddyProc**\n","metadata":{"id":"2IAI9o7vkqN5"}},{"cell_type":"code","source":"# Define columns of interest\ncolumns_of_interest = [\"Year\", \"Hour\", \"DoY\", \"NEE\", \"LE\", \"H\", \"u*\", \"qc_co2_flux\", \"qc_h2o_flux\",\n                       \"RN_1_1_1\", \"air_temperature\", \"RH_1_1_1\", \"SWIN_1_1_1\", \"TS_1_1_1\", \"VPD\"]\n\n# Subset the DataFrame with the columns of interest\nsubset_df = df[columns_of_interest]\ndf = subset_df.copy()        # copy\n#df.head()","metadata":{"id":"UpKKMJJGpMwT","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\n# Load your actual dataset\nsubset_df = subset_df  # Assuming the subset dataset is already loaded as 'subset_df'\n\n# Set the figure size\nplt.figure(figsize=(14, 14))\n\n# Get all parameters from the dataset\nparameters = [col for col in subset_df.columns if col not in ['Year', 'Season']]\nvalid_colors = sns.color_palette('husl', len(parameters))  # Dynamically generate enough colors\n\n# Iterate through each parameter to create a boxplot for outlier screening\nfor i, parameter in enumerate(parameters):\n    plt.subplot((len(parameters) + 1) // 2, 2, i + 1)  # Adjust grid layout based on number of parameters\n    sns.boxplot(y=subset_df[parameter], palette=[valid_colors[i]])\n    plt.title(parameter)\n    plt.ylabel(parameter)\n\nplt.tight_layout()\nplt.show()\n'''","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"id":"zk9IvlbmC7_Y","outputId":"8a4af965-2834-4fb5-b839-7d9162b13806","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 🧾 **12.2 Required header rows**","metadata":{"id":"-Jx5urWLDBBI"}},{"cell_type":"code","source":"# construct header rows for the resulting datafile, rename some columns\n# Make sure the best data quality is checked (for example RH_1_1_1 instead RH) from Biomet\nheader_rows = pd.DataFrame([len(df.columns)*['--']], columns=df.columns)\nheader_rows.rename(columns={\"SWIN_1_1_1\" : \"Rg\", \"RH_1_1_1\" : \"rH\",\"u*\": \"Ustar\",\n                          \"air_temperature\" : \"Tair\", \"TS_1_1_1\" : \"Tsoil\", \"RN_1_1_1\" : \"Net\" }, inplace=True)\nprint(header_rows)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PlNlIk3QC-E_","outputId":"c125cad0-e515-48ae-cf35-ccce108200ba","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 🧮 **12.3 Dataset for REddyProc**","metadata":{"id":"-y2_XRt6DFtO"}},{"cell_type":"code","source":"# Saving as a tab-separated text file with unit placeholders in the second row\nres_file_txt = \"AsiaFlux2025.txt\"\nheader_rows.to_csv(res_file_txt, sep='\\t', index=False)\ndf.to_csv(res_file_txt, mode='a', sep='\\t', na_rep='-9999', header=False, index=False)\n\n# Saving as a CSV file with unit placeholders in the second row\nres_file_csv = \"AsiaFlux2025.csv\"\nheader_rows.to_csv(res_file_csv, sep='\\t', index=False)\ndf.to_csv(res_file_csv, mode='a', sep='\\t', na_rep='-9999', header=False, index=False)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"YSPz5fVeDIdA","outputId":"5d847c50-a77b-4361-b5e7-2d7fbdd4ece3","trusted":true},"outputs":[],"execution_count":null}]}